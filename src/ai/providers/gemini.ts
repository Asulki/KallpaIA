
import "server-only";
import { GoogleGenerativeAI } from "@google/generative-ai";

export type Msg = { role: "system" | "user" | "assistant"; content: string };

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
const modelId = process.env.GEMINI_MODEL || "gemini-1.5-flash";

const SYSTEM = `Identidad: Eres KallpaWarmIA, mentora c√°lida para ni√±as y j√≥venes (12‚Äì17) en STEAM. Idiomas: espa√±ol y quechua.
Misi√≥n: Empoderar, corregir mitos y sesgos de g√©nero, y orientar con pasos pr√°cticos y seguros.

Saludo inicial (solo primer turno):
‚Äú¬°Rimaykullayki! Hola. Soy KallpaWarmIA, tu mentora digital para conquistar el mundo STEM. ¬øDeseas continuar en quechua o prefieres que hablemos en espa√±ol? Elige: [Espa√±ol] | [Quechua]‚Äù.

Reglas de estilo (todas las respuestas):
‚Ä¢ Brevedad: ‚â§120 palabras o 4 l√≠neas.
‚Ä¢ Estructura: 1) Empat√≠a (1 l√≠nea). 2) Sugerencia (m√°x. 2 vi√±etas). 3) 1 pregunta de cierre. 0‚Äì1 emoji.
‚Ä¢ Correcci√≥n de sesgos: si aparece des√°nimo/estereotipo (‚Äúeso es para chicos‚Äù), ofrece 1 referente mujer STEAM (nombre + 1 l√≠nea) y un paso accionable, sin tono negativo.
‚Ä¢ Seguridad/edad: sin datos personales; sugiere consultar a una persona adulta en decisiones complejas.
‚Ä¢ Idioma: mant√©n el elegido; si no es ES/QU, responde en espa√±ol y explica amablemente que solo hablas espa√±ol y quechua.
‚Ä¢ No repitas el saludo despu√©s del primer turno.

Few-shots mini (a√±ade detr√°s)

Mito (ES):
Usuario: ‚ÄúLa tecnolog√≠a es para chicos.‚Äù
Asistente: ‚ÄúEntiendo que lo oigas, y puede doler.
‚Ä¢ Prueba un micro-reto: bot√≥n que cambia texto (15‚Äì20 min).
‚Ä¢ Luego me lo muestras y te doy feedback.
Referente: Ada Lovelace program√≥ el primer algoritmo. ¬øTe animas a intentarlo? üòä‚Äù

Des√°nimo (ES):
Usuario: ‚ÄúSoy mala en mates.‚Äù
Asistente: ‚ÄúEs normal sentirse as√≠ al inicio.
‚Ä¢ 10 min con patrones (3 ejercicios).
‚Ä¢ Explica 1 en voz alta (aprendizaje activo).
Referente: Sof√≠a Kovalevskaya avanz√≥ practicando cada d√≠a. ¬øHacemos uno juntas?‚Äù

Idioma (QU):
Usuario: ‚ÄúAllin punchaw.‚Äù
Asistente: ‚ÄúKusikuyki!
‚Ä¢ 10 min: huk llank‚Äôana (suma/patrones).
‚Ä¢ Qhipa: rurasqayki qhawarichiy, feedback kurqusaq.
¬øImayna ruwasunchis?‚Äù`;

function splitHistory(messages: Msg[]) {
  // 1) ¬øya contest√≥ el asistente alguna vez?
  const hasAssistant = messages.some(m => m.role === "assistant");
  // 2) system externo (si lo usas adem√°s de systemInstruction)
  const sys = messages.find(m => m.role === "system")?.content;
  // 3) convierte historial para Gemini y separa el √∫ltimo user
  const history: { role:"user"|"model"; parts:{text:string}[] }[] = [];
  let lastUser = "";

  // Inserta system solo si existe (systemInstruction ya cubre el system global)
  if (sys) history.push({ role: "user", parts: [{ text: sys }] });

  messages.forEach((m, i) => {
    if (m.role === "system") return;
    if (m.role === "user") {
      if (i === messages.length - 1) lastUser = m.content;
      else history.push({ role: "user", parts: [{ text: m.content }] });
    } else {
      history.push({ role: "model", parts: [{ text: m.content }] });
    }
  });

  // Si es primer turno (sin respuestas previas), obliga al saludo
  const firstTurn = !hasAssistant;
  if (firstTurn) {
    lastUser =
      (lastUser ? lastUser + "\n\n" : "") +
      "PRIMER TURNO: Usa el saludo inicial EXACTO y espera elecci√≥n de idioma antes de seguir.";
  }
  if (!lastUser) lastUser = "Continuemos.";

  return { history, lastUser };
}

export async function chatOnceGemini(messages: Msg[], temperature = 0.6) {
  const { history, lastUser } = splitHistory(messages);
  const model = genAI.getGenerativeModel({
    model: modelId,
    systemInstruction: SYSTEM,
    generationConfig: { temperature, maxOutputTokens: 200, topP: 0.9, topK: 40 },
  });
  const chat = model.startChat({ history });
  const res = await chat.sendMessage(lastUser);
  return res.response.text();
}

export async function* chatStreamGemini(messages: Msg[], temperature = 0.6) {
  const { history, lastUser } = splitHistory(messages);
  const model = genAI.getGenerativeModel({
    model: modelId,
    systemInstruction: SYSTEM,
    generationConfig: { temperature, maxOutputTokens: 200, topP: 0.9, topK: 40 },
  });
  const chat = model.startChat({ history });
  const stream = await chat.sendMessageStream(lastUser);
  for await (const chunk of stream.stream) {
    const text = chunk.text();
    if (text) yield text;
  }
}
